{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "150b8965",
   "metadata": {},
   "source": [
    "# IBSA Pharmaceutical EDA Validation & Pipeline Setup\n",
    "\n",
    "This notebook validates the original IBSA_PoC_EDA.ipynb code and prepares the environment for feature engineering and machine learning modeling.\n",
    "\n",
    "## Objectives:\n",
    "1. **Validate Environment**: Ensure all required libraries and dependencies are available\n",
    "2. **Test Data Loading**: Verify access to all CSV files and data integrity\n",
    "3. **Execute EDA Components**: Run key analysis sections to confirm functionality\n",
    "4. **Prepare for ML Pipeline**: Set up data structures for feature engineering and modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd035d4d",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0fae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST: Let's discover what CSV files are actually available\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç DISCOVERING CSV FILES (No Spark needed yet)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get current directory and parent directories\n",
    "current_dir = Path(os.getcwd())\n",
    "parent_dir = current_dir.parent.parent  # Two levels up\n",
    "print(f\"üìÅ Current directory: {current_dir}\")\n",
    "print(f\"üìÇ Parent directory: {parent_dir}\")\n",
    "\n",
    "# Search for CSV files in multiple locations\n",
    "locations_to_search = [\n",
    "    current_dir,\n",
    "    current_dir.parent,\n",
    "    parent_dir,\n",
    "    Path(\"c:/Users/SandeepT/IBSA PoC V2/\")\n",
    "]\n",
    "\n",
    "all_csv_files = []\n",
    "print(f\"\\nüîç SEARCHING FOR CSV FILES:\")\n",
    "\n",
    "for location in locations_to_search:\n",
    "    if location.exists():\n",
    "        csv_files = list(location.glob(\"*.csv\"))\n",
    "        print(f\"\\nüìç Location: {location}\")\n",
    "        print(f\"   Found {len(csv_files)} CSV files:\")\n",
    "        \n",
    "        for csv_file in sorted(csv_files):\n",
    "            file_size = csv_file.stat().st_size / (1024*1024)  # Size in MB\n",
    "            print(f\"   üìÑ {csv_file.name} ({file_size:.1f} MB)\")\n",
    "            all_csv_files.append(str(csv_file))\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Location does not exist: {location}\")\n",
    "\n",
    "print(f\"\\nüìä TOTAL CSV FILES FOUND: {len(all_csv_files)}\")\n",
    "\n",
    "if all_csv_files:\n",
    "    print(f\"\\n‚úÖ CSV FILES READY FOR LOADING:\")\n",
    "    for i, file_path in enumerate(all_csv_files[:10], 1):  # Show first 10\n",
    "        file_name = Path(file_path).name\n",
    "        print(f\"   {i:2d}. {file_name}\")\n",
    "    if len(all_csv_files) > 10:\n",
    "        print(f\"   ... and {len(all_csv_files) - 10} more files\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  NO CSV FILES FOUND!\")\n",
    "    print(f\"   Please check if files are in the correct location\")\n",
    "    print(f\"   Expected location: {parent_dir}\")\n",
    "\n",
    "print(f\"\\nüéØ Next: We'll load these files using pandas (safer than Spark for testing)\")\n",
    "print(f\"üíæ NO DATABASE CREDENTIALS REQUIRED - Using CSV files only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590bc104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PySpark libraries imported successfully!\n",
      "‚úÖ Plotly imported successfully!\n",
      "‚úÖ SciPy imported successfully!\n",
      "\n",
      "üìã LIBRARY STATUS SUMMARY:\n",
      "‚úÖ Core libraries: pandas 2.2.2, numpy 1.26.4\n",
      "‚úÖ Visualization: matplotlib 3.8.4, seaborn 0.13.2\n",
      "üî• PySpark available: True\n",
      "üìä Plotly available: True\n",
      "üìà SciPy available: True\n",
      "üìÖ Analysis timestamp: 2025-09-26 14:11:24.722998\n",
      "\n",
      "üìã IBSA REPORTING TABLES TO LOAD:\n",
      "   1. call_activity_overview              ‚Üí Reporting_BI_CallActivity\n",
      "   2. call_attainment_summary             ‚Üí Reporting_BI_CallAttainment_Summary_TerritoryLevel\n",
      "   3. samples_trx_summary                 ‚Üí Reporting_BI_Trx_SampleSummary\n",
      "   4. samples_nrx_summary                 ‚Üí Reporting_BI_Nrx_SampleSummary\n",
      "   5. territory_calls_summary             ‚Üí Reporting_Bi_Territory_CallSummary\n",
      "   6. territory_samples_ll                ‚Üí Reporting_BI_Sample_LL_DTP\n",
      "   7. call_attainment_tiers               ‚Üí Reporting_BI_CallAttainment_Summary_Tier\n",
      "   8. ngd_overview                        ‚Üí Reporting_BI_NGD\n",
      "   9. prescriber_profile                  ‚Üí Reporting_BI_PrescriberProfile\n",
      "  10. prescriber_payment_summary          ‚Üí Reporting_BI_PrescriberOverview\n",
      "  11. prescriber_payment_plan_summary     ‚Üí Reporting_BI_PrescriberPaymentPlanSummary\n",
      "  12. prescriber_overview                 ‚Üí Reporting_BI_PrescriberOverview\n",
      "  13. territory_performance_summary       ‚Üí Reporting_BI_TerritoryPerformanceSummary\n",
      "  14. territory_performance               ‚Üí Reporting_BI_TerritoryPerformanceOverview\n",
      "  15. hcp_universe_live                   ‚Üí Reporting_Live_HCP_Universe\n",
      "\n",
      "üéØ TOTAL TABLES TO LOAD: 15\n",
      "üîç Next: We'll search for CSV files that match these table names\n",
      "üíæ No database credentials needed - using CSV files only\n"
     ]
    }
   ],
   "source": [
    "# System and File Operations (First - no dependencies)\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Core Data Science Libraries (Basic - usually available)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Try to import optional libraries\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.types import *\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.ml.feature import VectorAssembler, StandardScaler as SparkStandardScaler\n",
    "    from pyspark.ml.stat import Correlation\n",
    "    from pyspark.ml import Pipeline\n",
    "    SPARK_AVAILABLE = True\n",
    "    print(\"‚úÖ PySpark libraries imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  PySpark not available: {e}\")\n",
    "    print(\"üìã Please install PySpark: pip install pyspark\")\n",
    "    SPARK_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "    print(\"‚úÖ Plotly imported successfully!\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Plotly not available - using matplotlib only\")\n",
    "    PLOTLY_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from scipy import stats\n",
    "    SCIPY_AVAILABLE = True\n",
    "    print(\"‚úÖ SciPy imported successfully!\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  SciPy not available\")\n",
    "    SCIPY_AVAILABLE = False\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nüìã LIBRARY STATUS SUMMARY:\")\n",
    "print(f\"‚úÖ Core libraries: pandas {pd.__version__}, numpy {np.__version__}\")\n",
    "print(f\"‚úÖ Visualization: matplotlib {plt.matplotlib.__version__}, seaborn {sns.__version__}\")\n",
    "print(f\"üî• PySpark available: {SPARK_AVAILABLE}\")\n",
    "print(f\"üìä Plotly available: {PLOTLY_AVAILABLE}\")\n",
    "print(f\"üìà SciPy available: {SCIPY_AVAILABLE}\")\n",
    "print(f\"üìÖ Analysis timestamp: {datetime.now()}\")\n",
    "\n",
    "# Show exactly what we'll be loading\n",
    "print(f\"\\nüìã IBSA REPORTING TABLES TO LOAD:\")\n",
    "IBSA_REPORTING_TABLES = {\n",
    "    'call_activity_overview': 'Reporting_BI_CallActivity',\n",
    "    'call_attainment_summary': 'Reporting_BI_CallAttainment_Summary_TerritoryLevel', \n",
    "    'samples_trx_summary': 'Reporting_BI_Trx_SampleSummary',\n",
    "    'samples_nrx_summary': 'Reporting_BI_Nrx_SampleSummary',\n",
    "    'territory_calls_summary': 'Reporting_Bi_Territory_CallSummary',\n",
    "    'territory_samples_ll': 'Reporting_BI_Sample_LL_DTP',\n",
    "    'call_attainment_tiers': 'Reporting_BI_CallAttainment_Summary_Tier',\n",
    "    'ngd_overview': 'Reporting_BI_NGD',\n",
    "    'prescriber_profile': 'Reporting_BI_PrescriberProfile',\n",
    "    'prescriber_payment_summary': 'Reporting_BI_PrescriberOverview',\n",
    "    'prescriber_payment_plan_summary': 'Reporting_BI_PrescriberPaymentPlanSummary',\n",
    "    'prescriber_overview': 'Reporting_BI_PrescriberOverview',\n",
    "    'territory_performance_summary': 'Reporting_BI_TerritoryPerformanceSummary',\n",
    "    'territory_performance': 'Reporting_BI_TerritoryPerformanceOverview',\n",
    "    'hcp_universe_live': 'Reporting_Live_HCP_Universe'\n",
    "}\n",
    "\n",
    "for i, (key, table_name) in enumerate(IBSA_REPORTING_TABLES.items(), 1):\n",
    "    print(f\"  {i:2d}. {key:<35} ‚Üí {table_name}\")\n",
    "\n",
    "print(f\"\\nüéØ TOTAL TABLES TO LOAD: {len(IBSA_REPORTING_TABLES)}\")\n",
    "print(f\"üîç Next: We'll search for CSV files that match these table names\")\n",
    "print(f\"üíæ No database credentials needed - using CSV files only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43893211",
   "metadata": {},
   "source": [
    "## 2. Configure File Paths and Data Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb02762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session with optimized configuration for large datasets\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IBSA_Pharmaceutical_EDA\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbose output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"üî• Spark Session initialized successfully!\")\n",
    "print(f\"‚ú® Spark Version: {spark.version}\")\n",
    "print(f\"üéØ Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"üíæ Driver Memory: {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"üìä Shuffle Partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"‚ö° Arrow enabled: {spark.conf.get('spark.sql.execution.arrow.pyspark.enabled')}\")\n",
    "\n",
    "# IBSA Reporting Tables Configuration as per your requirements\n",
    "IBSA_REPORTING_TABLES = {\n",
    "    # Call & Activity Tables\n",
    "    'call_activity_overview': 'Reporting_BI_CallActivity',\n",
    "    'call_attainment_summary': 'Reporting_BI_CallAttainment_Summary_TerritoryLevel',\n",
    "    'samples_trx_summary': 'Reporting_BI_Trx_SampleSummary',\n",
    "    'samples_nrx_summary': 'Reporting_BI_Nrx_SampleSummary',\n",
    "    'territory_calls_summary': 'Reporting_Bi_Territory_CallSummary',\n",
    "    'territory_samples_ll': 'Reporting_BI_Sample_LL_DTP',\n",
    "    'call_attainment_tiers': 'Reporting_BI_CallAttainment_Summary_Tier',\n",
    "    \n",
    "    # New/Growth/Decliner Analysis\n",
    "    'ngd_overview': 'Reporting_BI_NGD',\n",
    "    \n",
    "    # Prescriber Intelligence Tables\n",
    "    'prescriber_profile': 'Reporting_BI_PrescriberProfile',\n",
    "    'prescriber_payment_summary': 'Reporting_BI_PrescriberOverview',\n",
    "    'prescriber_payment_plan_summary': 'Reporting_BI_PrescriberPaymentPlanSummary',\n",
    "    'prescriber_overview': 'Reporting_BI_PrescriberOverview',\n",
    "    \n",
    "    # Territory Performance Tables\n",
    "    'territory_performance_summary': 'Reporting_BI_TerritoryPerformanceSummary',\n",
    "    'territory_performance': 'Reporting_BI_TerritoryPerformanceOverview',\n",
    "    \n",
    "    # HCP Universe (Critical for analysis)\n",
    "    'hcp_universe_live': 'Reporting_Live_HCP_Universe'\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã IBSA Reporting Tables to Process:\")\n",
    "for i, (key, table) in enumerate(IBSA_REPORTING_TABLES.items(), 1):\n",
    "    print(f\"  {i:2d}. {key:<30} ‚Üí {table}\")\n",
    "\n",
    "# Configure file paths\n",
    "current_dir = Path(os.getcwd())\n",
    "data_dir = current_dir.parent.parent  # Go up two levels to find CSV files\n",
    "print(f\"\\nüìÅ Current working directory: {current_dir}\")\n",
    "print(f\"üìÇ Data directory: {data_dir}\")\n",
    "\n",
    "# Load available CSV files and map to reporting tables\n",
    "available_files = {}\n",
    "csv_files = list(data_dir.glob(\"*.csv\"))\n",
    "print(f\"\\nüîç Found {len(csv_files)} CSV files in data directory:\")\n",
    "\n",
    "for file_path in sorted(csv_files):\n",
    "    file_key = file_path.stem.lower()\n",
    "    available_files[file_key] = str(file_path)\n",
    "    print(f\"  üìÑ {file_path.name}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to process {len(available_files)} data files with Spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174974c8",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Validation with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f006737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBSA EDA - What We're Loading\n",
    "print(\"üéØ IBSA REPORTING TABLES TO LOAD:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# The 15 specific IBSA reporting tables\n",
    "tables = [\n",
    "    \"1.  Call Activity\",\n",
    "    \"2.  Call Attainment\", \n",
    "    \"3.  Territory Performance\",\n",
    "    \"4.  Prescriber Profile\",\n",
    "    \"5.  HCP Universe\", \n",
    "    \"6.  TRx Data\",\n",
    "    \"7.  NRx Data\",\n",
    "    \"8.  Sample Data\",\n",
    "    \"9.  NGD Analysis\",\n",
    "    \"10. Market Share\",\n",
    "    \"11. Competitor Data\", \n",
    "    \"12. Payment Methods\",\n",
    "    \"13. Geography Mapping\",\n",
    "    \"14. Product Analysis\",\n",
    "    \"15. Reporting Live HCP Universe\"\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    print(table)\n",
    "\n",
    "print(f\"\\nüìÑ CSV FILES:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚Ä¢ Looking for IBSA_*.csv files in parent directory\")\n",
    "print(\"‚Ä¢ Expected: IBSA_NRx_Enhanced.csv, IBSA_HCP_Universe_Live.csv, etc.\")\n",
    "\n",
    "print(f\"\\nüîê CREDENTIALS:\")\n",
    "print(\"=\" * 50) \n",
    "print(\"‚Ä¢ Data Source: CSV files (no database connection)\")\n",
    "print(\"‚Ä¢ Processing: Apache Spark for large datasets\")\n",
    "print(\"‚Ä¢ Memory: Optimized configuration\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to load and analyze pharmaceutical data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad077e3d",
   "metadata": {},
   "source": [
    "## 3.5 Primary/Foreign Key Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary/Foreign Key Relationship Analysis for IBSA Reporting Tables\n",
    "def analyze_table_relationships(dataframes_dict):\n",
    "    \"\"\"\n",
    "    Analyze potential PK/FK relationships between IBSA reporting tables\n",
    "    \"\"\"\n",
    "    print(\"üîó Analyzing Primary/Foreign Key Relationships\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    relationships = {}\n",
    "    common_join_keys = []\n",
    "    \n",
    "    # Common pharmaceutical industry key patterns\n",
    "    key_patterns = {\n",
    "        'prescriber_keys': ['prescriber_id', 'hcp_id', 'provider_id', 'doctor_id', 'physician_id'],\n",
    "        'territory_keys': ['territory_id', 'territory_code', 'region_id', 'area_id'],\n",
    "        'product_keys': ['product_id', 'drug_id', 'ndc', 'brand_id', 'product_code'],\n",
    "        'call_keys': ['call_id', 'activity_id', 'interaction_id'],\n",
    "        'date_keys': ['date', 'call_date', 'prescription_date', 'activity_date'],\n",
    "        'geography_keys': ['zip', 'zip_code', 'state', 'city', 'county'],\n",
    "        'sample_keys': ['sample_id', 'lot_id', 'batch_id']\n",
    "    }\n",
    "    \n",
    "    # Analyze each table for potential keys\n",
    "    table_keys = {}\n",
    "    \n",
    "    for table_name, table_info in dataframes_dict.items():\n",
    "        df = table_info['dataframe']\n",
    "        columns = df.columns\n",
    "        \n",
    "        print(f\"\\nüìä Analyzing: {table_name}\")\n",
    "        print(f\"   Table: {table_info['table_name']}\")\n",
    "        \n",
    "        found_keys = {}\n",
    "        \n",
    "        # Look for key patterns in column names\n",
    "        for key_type, key_list in key_patterns.items():\n",
    "            matching_cols = []\n",
    "            for col in columns:\n",
    "                col_lower = col.lower()\n",
    "                if any(key_pattern in col_lower for key_pattern in key_list):\n",
    "                    matching_cols.append(col)\n",
    "            \n",
    "            if matching_cols:\n",
    "                found_keys[key_type] = matching_cols\n",
    "                print(f\"   üîë {key_type}: {matching_cols}\")\n",
    "        \n",
    "        # Store for relationship analysis\n",
    "        table_keys[table_name] = {\n",
    "            'keys': found_keys,\n",
    "            'all_columns': columns,\n",
    "            'row_count': table_info['row_count']\n",
    "        }\n",
    "        \n",
    "        # Check for potential primary keys (high uniqueness)\n",
    "        potential_pks = []\n",
    "        for col in columns[:10]:  # Check first 10 columns for performance\n",
    "            try:\n",
    "                unique_count = df.select(col).distinct().count()\n",
    "                total_count = table_info['row_count']\n",
    "                uniqueness_ratio = unique_count / total_count if total_count > 0 else 0\n",
    "                \n",
    "                if uniqueness_ratio > 0.95:  # 95% unique values\n",
    "                    potential_pks.append((col, uniqueness_ratio))\n",
    "                    \n",
    "            except Exception:\n",
    "                continue  # Skip columns that can't be analyzed\n",
    "        \n",
    "        if potential_pks:\n",
    "            print(f\"   üè∑Ô∏è  Potential PKs: {[(col, f'{ratio:.2%}') for col, ratio in potential_pks]}\")\n",
    "    \n",
    "    # Find common keys across tables for potential joins\n",
    "    print(f\"\\nüîç Cross-Table Relationship Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    all_key_types = set()\n",
    "    for table_keys_info in table_keys.values():\n",
    "        all_key_types.update(table_keys_info['keys'].keys())\n",
    "    \n",
    "    for key_type in all_key_types:\n",
    "        tables_with_key = []\n",
    "        for table_name, table_info in table_keys.items():\n",
    "            if key_type in table_info['keys']:\n",
    "                tables_with_key.append((table_name, table_info['keys'][key_type]))\n",
    "        \n",
    "        if len(tables_with_key) > 1:\n",
    "            print(f\"\\nüîó {key_type.upper()} - Found in {len(tables_with_key)} tables:\")\n",
    "            for table_name, key_cols in tables_with_key:\n",
    "                print(f\"   üìã {table_name}: {key_cols}\")\n",
    "            common_join_keys.append({\n",
    "                'key_type': key_type,\n",
    "                'tables': tables_with_key\n",
    "            })\n",
    "    \n",
    "    return table_keys, common_join_keys\n",
    "\n",
    "# HCP Universe Analysis (Critical as per your requirements)\n",
    "def analyze_hcp_universe(dataframes_dict):\n",
    "    \"\"\"\n",
    "    Special analysis for Reporting_Live_HCP_Universe table - why it's needed\n",
    "    \"\"\"\n",
    "    print(f\"\\nüè• HCP Universe Analysis - Why It's Critical\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    hcp_table = None\n",
    "    hcp_key = None\n",
    "    \n",
    "    # Find HCP Universe table\n",
    "    for key, info in dataframes_dict.items():\n",
    "        if 'hcp' in key.lower() and 'universe' in key.lower():\n",
    "            hcp_table = info['dataframe']\n",
    "            hcp_key = key\n",
    "            break\n",
    "    \n",
    "    if hcp_table is None:\n",
    "        print(\"‚ö†Ô∏è  HCP Universe table not found - this is critical for:\")\n",
    "        print(\"   üéØ Healthcare Provider master data\")\n",
    "        print(\"   üìç Geographic analysis and territory mapping\")\n",
    "        print(\"   üë®‚Äç‚öïÔ∏è Prescriber profiling and segmentation\")\n",
    "        print(\"   üè• Practice type and specialty analysis\")\n",
    "        print(\"   üìä Market sizing and opportunity assessment\")\n",
    "        print(\"   üîó Primary key for joining with other tables\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ Found HCP Universe table: {hcp_key}\")\n",
    "    print(f\"üìä Shape: {dataframes_dict[hcp_key]['row_count']:,} rows √ó {dataframes_dict[hcp_key]['column_count']} columns\")\n",
    "    \n",
    "    # Analyze HCP Universe structure\n",
    "    columns = hcp_table.columns\n",
    "    print(f\"\\nüîç HCP Universe Column Analysis:\")\n",
    "    \n",
    "    # Categorize columns by purpose\n",
    "    column_categories = {\n",
    "        'identifier_cols': [col for col in columns if any(term in col.lower() for term in ['id', 'key', 'code'])],\n",
    "        'demographic_cols': [col for col in columns if any(term in col.lower() for term in ['name', 'type', 'specialty'])],\n",
    "        'geographic_cols': [col for col in columns if any(term in col.lower() for term in ['address', 'zip', 'state', 'city', 'territory'])],\n",
    "        'classification_cols': [col for col in columns if any(term in col.lower() for term in ['tier', 'segment', 'class', 'category'])]\n",
    "    }\n",
    "    \n",
    "    for category, cols in column_categories.items():\n",
    "        if cols:\n",
    "            print(f\"   {category.replace('_', ' ').title()}: {cols[:5]}{'...' if len(cols) > 5 else ''}\")\n",
    "    \n",
    "    print(f\"\\nüí° HCP Universe Importance:\")\n",
    "    print(f\"   üéØ Serves as master healthcare provider reference\")\n",
    "    print(f\"   üîó Primary join key for all prescriber-related analysis\")\n",
    "    print(f\"   üìç Enables geographic and territory-based insights\")\n",
    "    print(f\"   üë• Essential for prescriber segmentation and targeting\")\n",
    "    print(f\"   üìä Foundation for market share and competitive analysis\")\n",
    "    \n",
    "    return hcp_table, column_categories\n",
    "\n",
    "# Execute relationship analysis\n",
    "if reporting_dataframes:\n",
    "    table_relationships, join_keys = analyze_table_relationships(reporting_dataframes)\n",
    "    hcp_analysis = analyze_hcp_universe(reporting_dataframes)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No tables loaded - skipping relationship analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b4386",
   "metadata": {},
   "source": [
    "## 4. Spark-Based Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a78a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive EDA Analysis for ALL IBSA Reporting Tables\n",
    "def comprehensive_ibsa_eda(dataframes_dict):\n",
    "    \"\"\"\n",
    "    Complete EDA analysis for all IBSA reporting tables as per requirements\n",
    "    \"\"\"\n",
    "    print(\"üî¨ COMPREHENSIVE IBSA REPORTING TABLES EDA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Group tables by business function\n",
    "    table_groups = {\n",
    "        'Call Activity & Attainment': ['call_activity_overview', 'call_attainment_summary', \n",
    "                                      'territory_calls_summary', 'call_attainment_tiers'],\n",
    "        'Prescription & Samples': ['samples_trx_summary', 'samples_nrx_summary', 'territory_samples_ll'],\n",
    "        'Healthcare Provider Intelligence': ['prescriber_profile', 'prescriber_payment_summary', \n",
    "                                           'prescriber_payment_plan_summary', 'prescriber_overview', 'hcp_universe_live'],\n",
    "        'Territory Performance': ['territory_performance_summary', 'territory_performance'],\n",
    "        'Growth & Market Analysis': ['ngd_overview']\n",
    "    }\n",
    "    \n",
    "    eda_results = {}\n",
    "    \n",
    "    for group_name, table_keys in table_groups.items():\n",
    "        print(f\"\\nüè∑Ô∏è  ANALYZING: {group_name.upper()}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        group_results = {}\n",
    "        \n",
    "        for table_key in table_keys:\n",
    "            if table_key in dataframes_dict:\n",
    "                print(f\"\\nüìä Table: {table_key}\")\n",
    "                print(f\"    Reporting Table: {dataframes_dict[table_key]['table_name']}\")\n",
    "                \n",
    "                df = dataframes_dict[table_key]['dataframe']\n",
    "                result = perform_detailed_table_analysis(df, table_key, dataframes_dict[table_key])\n",
    "                group_results[table_key] = result\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  Missing: {table_key}\")\n",
    "                # Create sample data for missing critical tables\n",
    "                if table_key in ['hcp_universe_live', 'prescriber_profile']:\n",
    "                    print(f\"   üîß Creating sample data for critical table: {table_key}\")\n",
    "                    sample_df = create_sample_table_data(table_key)\n",
    "                    if sample_df:\n",
    "                        group_results[table_key] = {'status': 'sample_created', 'dataframe': sample_df}\n",
    "        \n",
    "        eda_results[group_name] = group_results\n",
    "    \n",
    "    return eda_results\n",
    "\n",
    "def perform_detailed_table_analysis(df, table_name, table_info):\n",
    "    \"\"\"\n",
    "    Detailed analysis for each reporting table\n",
    "    \"\"\"\n",
    "    print(f\"    üìà Shape: {table_info['row_count']:,} rows √ó {table_info['column_count']} columns\")\n",
    "    \n",
    "    # Column analysis\n",
    "    numeric_cols = [col for col, dtype in df.dtypes if dtype in ['int', 'bigint', 'double', 'float']]\n",
    "    string_cols = [col for col, dtype in df.dtypes if dtype == 'string']\n",
    "    date_cols = [col for col in df.columns if any(term in col.lower() for term in ['date', 'time', 'timestamp'])]\n",
    "    \n",
    "    print(f\"    üî¢ Numeric columns: {len(numeric_cols)}\")\n",
    "    print(f\"    üè∑Ô∏è  String columns: {len(string_cols)}\")\n",
    "    print(f\"    üìÖ Date columns: {len(date_cols)}\")\n",
    "    \n",
    "    # Business-specific analysis based on table type\n",
    "    business_insights = {}\n",
    "    \n",
    "    if 'call' in table_name.lower():\n",
    "        business_insights = analyze_call_activity_table(df, table_name)\n",
    "    elif 'prescriber' in table_name.lower() or 'hcp' in table_name.lower():\n",
    "        business_insights = analyze_prescriber_table(df, table_name)\n",
    "    elif 'territory' in table_name.lower():\n",
    "        business_insights = analyze_territory_table(df, table_name)\n",
    "    elif 'sample' in table_name.lower() or 'trx' in table_name.lower() or 'nrx' in table_name.lower():\n",
    "        business_insights = analyze_prescription_table(df, table_name)\n",
    "    elif 'ngd' in table_name.lower():\n",
    "        business_insights = analyze_ngd_table(df, table_name)\n",
    "    \n",
    "    return {\n",
    "        'row_count': table_info['row_count'],\n",
    "        'column_count': table_info['column_count'],\n",
    "        'numeric_cols': len(numeric_cols),\n",
    "        'string_cols': len(string_cols),\n",
    "        'date_cols': len(date_cols),\n",
    "        'business_insights': business_insights,\n",
    "        'columns': df.columns[:10]  # First 10 columns for reference\n",
    "    }\n",
    "\n",
    "# Business-specific analysis functions\n",
    "def analyze_call_activity_table(df, table_name):\n",
    "    \"\"\"Analyze call activity and attainment tables\"\"\"\n",
    "    print(f\"    üìû Call Activity Analysis:\")\n",
    "    \n",
    "    insights = {}\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Look for call metrics\n",
    "    call_metrics = [col for col in columns if any(term in col.lower() for term in \n",
    "                   ['calls', 'visits', 'interactions', 'planned', 'completed', 'attainment'])]\n",
    "    \n",
    "    if call_metrics:\n",
    "        print(f\"      üéØ Call Metrics Found: {call_metrics[:5]}\")\n",
    "        insights['call_metrics'] = call_metrics\n",
    "        \n",
    "        # Sample statistics for first metric (using limit to avoid memory issues)\n",
    "        try:\n",
    "            first_metric = call_metrics[0]\n",
    "            stats = df.select(first_metric).describe().collect()\n",
    "            print(f\"      üìä {first_metric} Statistics: {[(row['summary'], row[first_metric]) for row in stats]}\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Territory analysis\n",
    "    territory_cols = [col for col in columns if any(term in col.lower() for term in ['territory', 'region', 'area'])]\n",
    "    if territory_cols:\n",
    "        print(f\"      üè¢ Territory Columns: {territory_cols[:3]}\")\n",
    "        insights['territory_cols'] = territory_cols\n",
    "    \n",
    "    return insights\n",
    "\n",
    "def analyze_prescriber_table(df, table_name):\n",
    "    \"\"\"Analyze prescriber and HCP tables\"\"\"\n",
    "    print(f\"    üë®‚Äç‚öïÔ∏è Healthcare Provider Analysis:\")\n",
    "    \n",
    "    insights = {}\n",
    "    columns = df.columns\n",
    "    \n",
    "    # HCP identifiers\n",
    "    hcp_ids = [col for col in columns if any(term in col.lower() for term in ['hcp', 'prescriber', 'provider', 'doctor'])]\n",
    "    if hcp_ids:\n",
    "        print(f\"      üÜî HCP Identifiers: {hcp_ids[:3]}\")\n",
    "        insights['hcp_identifiers'] = hcp_ids\n",
    "    \n",
    "    # Specialty analysis\n",
    "    specialty_cols = [col for col in columns if any(term in col.lower() for term in ['specialty', 'type', 'classification'])]\n",
    "    if specialty_cols:\n",
    "        print(f\"      üè• Specialty Columns: {specialty_cols[:3]}\")\n",
    "        insights['specialty_cols'] = specialty_cols\n",
    "        \n",
    "        # Top specialties\n",
    "        try:\n",
    "            first_specialty = specialty_cols[0]\n",
    "            top_specialties = df.groupBy(first_specialty).count().orderBy(F.desc(\"count\")).limit(5).collect()\n",
    "            print(f\"      üèÜ Top Specialties: {[(row[first_specialty], row['count']) for row in top_specialties]}\")\n",
    "            insights['top_specialties'] = top_specialties\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Payment/Plan analysis\n",
    "    payment_cols = [col for col in columns if any(term in col.lower() for term in ['payment', 'plan', 'payer', 'insurance'])]\n",
    "    if payment_cols:\n",
    "        print(f\"      üí∞ Payment Columns: {payment_cols[:3]}\")\n",
    "        insights['payment_cols'] = payment_cols\n",
    "    \n",
    "    return insights\n",
    "\n",
    "def analyze_territory_table(df, table_name):\n",
    "    \"\"\"Analyze territory performance tables\"\"\"\n",
    "    print(f\"    üè¢ Territory Performance Analysis:\")\n",
    "    \n",
    "    insights = {}\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Performance metrics\n",
    "    performance_cols = [col for col in columns if any(term in col.lower() for term in \n",
    "                       ['performance', 'achievement', 'target', 'goal', 'quota', 'sales'])]\n",
    "    if performance_cols:\n",
    "        print(f\"      üìà Performance Metrics: {performance_cols[:5]}\")\n",
    "        insights['performance_metrics'] = performance_cols\n",
    "    \n",
    "    # Geographic columns\n",
    "    geo_cols = [col for col in columns if any(term in col.lower() for term in \n",
    "               ['zip', 'state', 'city', 'region', 'territory', 'area'])]\n",
    "    if geo_cols:\n",
    "        print(f\"      üìç Geographic Columns: {geo_cols[:5]}\")\n",
    "        insights['geographic_cols'] = geo_cols\n",
    "    \n",
    "    return insights\n",
    "\n",
    "def analyze_prescription_table(df, table_name):\n",
    "    \"\"\"Analyze prescription and sample tables\"\"\"\n",
    "    print(f\"    üíä Prescription/Sample Analysis:\")\n",
    "    \n",
    "    insights = {}\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Prescription metrics\n",
    "    rx_cols = [col for col in columns if any(term in col.lower() for term in \n",
    "              ['nrx', 'trx', 'prescription', 'units', 'quantity', 'volume'])]\n",
    "    if rx_cols:\n",
    "        print(f\"      üìä Prescription Metrics: {rx_cols[:5]}\")\n",
    "        insights['prescription_metrics'] = rx_cols\n",
    "    \n",
    "    # Sample metrics\n",
    "    sample_cols = [col for col in columns if any(term in col.lower() for term in \n",
    "                  ['sample', 'units_given', 'quantity_dispensed'])]\n",
    "    if sample_cols:\n",
    "        print(f\"      üéÅ Sample Metrics: {sample_cols[:5]}\")\n",
    "        insights['sample_metrics'] = sample_cols\n",
    "    \n",
    "    # Product information\n",
    "    product_cols = [col for col in columns if any(term in col.lower() for term in \n",
    "                   ['product', 'brand', 'drug', 'ndc'])]\n",
    "    if product_cols:\n",
    "        print(f\"      üè∑Ô∏è  Product Columns: {product_cols[:3]}\")\n",
    "        insights['product_cols'] = product_cols\n",
    "    \n",
    "    return insights\n",
    "\n",
    "def analyze_ngd_table(df, table_name):\n",
    "    \"\"\"Analyze New/Growth/Decliner tables\"\"\"\n",
    "    print(f\"    üìà New/Growth/Decliner Analysis:\")\n",
    "    \n",
    "    insights = {}\n",
    "    columns = df.columns\n",
    "    \n",
    "    # NGD classifications\n",
    "    ngd_cols = [col for col in columns if any(term in col.lower() for term in \n",
    "               ['new', 'growth', 'decline', 'writer', 'segment'])]\n",
    "    if ngd_cols:\n",
    "        print(f\"      üéØ NGD Classifications: {ngd_cols[:5]}\")\n",
    "        insights['ngd_classifications'] = ngd_cols\n",
    "    \n",
    "    return insights\n",
    "\n",
    "def create_sample_table_data(table_key):\n",
    "    \"\"\"Create sample data for missing critical tables\"\"\"\n",
    "    print(f\"    üîß Generating sample data for: {table_key}\")\n",
    "    \n",
    "    if table_key == 'hcp_universe_live':\n",
    "        # Create sample HCP Universe\n",
    "        sample_data = [\n",
    "            (\"HCP001\", \"Dr. John Smith\", \"Cardiology\", \"Primary Care\", \"12345\", \"CA\", \"Los Angeles\", \"TERR001\"),\n",
    "            (\"HCP002\", \"Dr. Jane Doe\", \"Endocrinology\", \"Specialty\", \"67890\", \"TX\", \"Houston\", \"TERR002\"),\n",
    "            (\"HCP003\", \"Dr. Mike Johnson\", \"Family Medicine\", \"Primary Care\", \"11111\", \"NY\", \"New York\", \"TERR003\")\n",
    "        ]\n",
    "        \n",
    "        columns = [\"hcp_id\", \"provider_name\", \"specialty\", \"provider_type\", \"zip_code\", \"state\", \"city\", \"territory_id\"]\n",
    "        \n",
    "    elif table_key == 'prescriber_profile':\n",
    "        # Create sample Prescriber Profile\n",
    "        sample_data = [\n",
    "            (\"HCP001\", \"High Volume\", \"Tier 1\", 150, 25, 1200),\n",
    "            (\"HCP002\", \"Medium Volume\", \"Tier 2\", 85, 15, 800),\n",
    "            (\"HCP003\", \"Low Volume\", \"Tier 3\", 45, 8, 400)\n",
    "        ]\n",
    "        \n",
    "        columns = [\"hcp_id\", \"volume_segment\", \"tier\", \"monthly_nrx\", \"sample_affinity\", \"total_patients\"]\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # Create Spark DataFrame\n",
    "    sample_df = spark.createDataFrame(sample_data, columns)\n",
    "    print(f\"    ‚úÖ Sample data created with {len(sample_data)} rows\")\n",
    "    \n",
    "    return sample_df\n",
    "\n",
    "# Execute comprehensive EDA analysis\n",
    "if reporting_dataframes:\n",
    "    comprehensive_results = comprehensive_ibsa_eda(reporting_dataframes)\n",
    "    print(f\"\\nüéâ Comprehensive EDA Analysis Complete!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No tables loaded for analysis\")\n",
    "    # Create sample data for demonstration\n",
    "    print(\"üîß Creating sample data for demonstration...\")\n",
    "    sample_reporting_data = {}\n",
    "    \n",
    "    for table_key in ['hcp_universe_live', 'prescriber_profile']:\n",
    "        sample_df = create_sample_table_data(table_key)\n",
    "        if sample_df:\n",
    "            sample_reporting_data[table_key] = {\n",
    "                'dataframe': sample_df,\n",
    "                'table_name': f'Sample_{table_key}',\n",
    "                'row_count': sample_df.count(),\n",
    "                'column_count': len(sample_df.columns)\n",
    "            }\n",
    "    \n",
    "    if sample_reporting_data:\n",
    "        comprehensive_results = comprehensive_ibsa_eda(sample_reporting_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514455a",
   "metadata": {},
   "source": [
    "## 5. Pharmaceutical Market Intelligence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pharmaceutical-specific analysis using Spark\n",
    "def analyze_prescriber_patterns(df, df_name):\n",
    "    \"\"\"\n",
    "    Analyze Healthcare Provider (HCP) prescribing patterns using Spark\n",
    "    \"\"\"\n",
    "    print(f\"\\nüíä Healthcare Provider Analysis: {df_name}\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Look for common pharmaceutical columns\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Identify key columns (common in pharma datasets)\n",
    "    prescriber_cols = [col for col in columns if any(term in col.lower() for term in ['prescriber', 'hcp', 'provider', 'doctor', 'physician'])]\n",
    "    product_cols = [col for col in columns if any(term in col.lower() for term in ['product', 'drug', 'ndc', 'brand'])]\n",
    "    volume_cols = [col for col in columns if any(term in col.lower() for term in ['nrx', 'trx', 'volume', 'units', 'quantity'])]\n",
    "    territory_cols = [col for col in columns if any(term in col.lower() for term in ['territory', 'region', 'zip', 'state'])]\n",
    "    \n",
    "    print(f\"üîç Identified Column Categories:\")\n",
    "    print(f\"  üë®‚Äç‚öïÔ∏è  Prescriber columns: {len(prescriber_cols)} - {prescriber_cols[:3]}...\")\n",
    "    print(f\"  üíä Product columns: {len(product_cols)} - {product_cols[:3]}...\")\n",
    "    print(f\"  üìä Volume columns: {len(volume_cols)} - {volume_cols[:3]}...\")\n",
    "    print(f\"  üìç Territory columns: {len(territory_cols)} - {territory_cols[:3]}...\")\n",
    "    \n",
    "    # High-level aggregations (efficient with Spark)\n",
    "    total_records = df.count()\n",
    "    \n",
    "    # Sample analysis for visualization\n",
    "    if total_records > 0:\n",
    "        # Get unique counts efficiently\n",
    "        if prescriber_cols:\n",
    "            unique_prescribers = df.select(prescriber_cols[0]).distinct().count() if prescriber_cols else 0\n",
    "            print(f\"  üë• Unique prescribers: {unique_prescribers:,}\")\n",
    "        \n",
    "        if product_cols:\n",
    "            unique_products = df.select(product_cols[0]).distinct().count() if product_cols else 0\n",
    "            print(f\"  üè∑Ô∏è  Unique products: {unique_products:,}\")\n",
    "        \n",
    "        if territory_cols:\n",
    "            unique_territories = df.select(territory_cols[0]).distinct().count() if territory_cols else 0\n",
    "            print(f\"  üåç Unique territories: {unique_territories:,}\")\n",
    "        \n",
    "        # Volume analysis (if available)\n",
    "        if volume_cols:\n",
    "            vol_col = volume_cols[0]\n",
    "            try:\n",
    "                volume_stats = df.select(vol_col).describe().toPandas()\n",
    "                print(f\"\\nüìà Volume Statistics ({vol_col}):\")\n",
    "                for _, row in volume_stats.iterrows():\n",
    "                    print(f\"  {row['summary']}: {row[vol_col]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Could not analyze volume column: {str(e)}\")\n",
    "\n",
    "# Function for competitive analysis\n",
    "def analyze_market_competition(df, df_name):\n",
    "    \"\"\"\n",
    "    Analyze competitive landscape using Spark\n",
    "    \"\"\"\n",
    "    print(f\"\\nüèÜ Market Competition Analysis: {df_name}\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Look for competitor/brand columns\n",
    "    brand_cols = [col for col in df.columns if any(term in col.lower() for term in ['brand', 'competitor', 'company', 'manufacturer'])]\n",
    "    share_cols = [col for col in df.columns if any(term in col.lower() for term in ['share', 'market', 'percentage', '%'])]\n",
    "    \n",
    "    if brand_cols:\n",
    "        print(f\"üè∑Ô∏è  Brand/Competitor columns found: {brand_cols[:3]}...\")\n",
    "        \n",
    "        # Top brands by frequency\n",
    "        brand_col = brand_cols[0]\n",
    "        try:\n",
    "            top_brands = df.groupBy(brand_col).count().orderBy(F.desc(\"count\")).limit(10).toPandas()\n",
    "            print(f\"\\nü•á Top Brands/Competitors ({brand_col}):\")\n",
    "            for i, row in top_brands.iterrows():\n",
    "                print(f\"  {i+1:2d}. {row[brand_col]}: {row['count']:,} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Error analyzing brands: {str(e)}\")\n",
    "    \n",
    "    if share_cols:\n",
    "        print(f\"üìä Market share columns found: {share_cols[:3]}...\")\n",
    "\n",
    "# Analyze available datasets\n",
    "print(\"üî¨ Starting Pharmaceutical Intelligence Analysis...\")\n",
    "\n",
    "for dataset_name, df in spark_dfs.items():\n",
    "    try:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìä ANALYZING: {dataset_name.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Basic pharmaceutical analysis\n",
    "        analyze_prescriber_patterns(df, dataset_name)\n",
    "        \n",
    "        # Competition analysis\n",
    "        analyze_market_competition(df, dataset_name)\n",
    "        \n",
    "        # Memory management - unpersist if not needed immediately\n",
    "        # df.unpersist() # Uncomment if memory is tight\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing {dataset_name}: {str(e)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Pharmaceutical Intelligence Analysis Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a38e8",
   "metadata": {},
   "source": [
    "## 6. Spark-Based Visualizations (Memory Efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffc6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBSA Pharmaceutical Business Intelligence Visualizations\n",
    "def create_ibsa_business_visualizations(dataframes_dict, max_categories=15):\n",
    "    \"\"\"\n",
    "    Create business-focused visualizations for IBSA reporting tables\n",
    "    \"\"\"\n",
    "    print(\"üìà Creating IBSA Business Intelligence Visualizations\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    viz_results = {}\n",
    "    \n",
    "    # 1. Call Activity Performance Dashboard\n",
    "    call_tables = [k for k in dataframes_dict.keys() if 'call' in k.lower()]\n",
    "    if call_tables:\n",
    "        print(f\"\\nüìû CALL ACTIVITY PERFORMANCE\")\n",
    "        for table_key in call_tables:\n",
    "            df = dataframes_dict[table_key]['dataframe']\n",
    "            create_call_activity_viz(df, table_key)\n",
    "    \n",
    "    # 2. Territory Performance Analysis\n",
    "    territory_tables = [k for k in dataframes_dict.keys() if 'territory' in k.lower()]\n",
    "    if territory_tables:\n",
    "        print(f\"\\nüè¢ TERRITORY PERFORMANCE ANALYSIS\")\n",
    "        for table_key in territory_tables:\n",
    "            df = dataframes_dict[table_key]['dataframe']\n",
    "            create_territory_performance_viz(df, table_key)\n",
    "    \n",
    "    # 3. Prescriber Intelligence Dashboard\n",
    "    prescriber_tables = [k for k in dataframes_dict.keys() if any(term in k.lower() for term in ['prescriber', 'hcp'])]\n",
    "    if prescriber_tables:\n",
    "        print(f\"\\nüë®‚Äç‚öïÔ∏è HEALTHCARE PROVIDER INTELLIGENCE\")\n",
    "        for table_key in prescriber_tables:\n",
    "            df = dataframes_dict[table_key]['dataframe']\n",
    "            create_prescriber_intelligence_viz(df, table_key)\n",
    "    \n",
    "    # 4. Prescription & Sample Performance\n",
    "    rx_tables = [k for k in dataframes_dict.keys() if any(term in k.lower() for term in ['trx', 'nrx', 'sample'])]\n",
    "    if rx_tables:\n",
    "        print(f\"\\nüíä PRESCRIPTION & SAMPLE PERFORMANCE\")\n",
    "        for table_key in rx_tables:\n",
    "            df = dataframes_dict[table_key]['dataframe']\n",
    "            create_prescription_sample_viz(df, table_key)\n",
    "    \n",
    "    # 5. New/Growth/Decliner Analysis\n",
    "    ngd_tables = [k for k in dataframes_dict.keys() if 'ngd' in k.lower()]\n",
    "    if ngd_tables:\n",
    "        print(f\"\\nüìà NEW/GROWTH/DECLINER ANALYSIS\")\n",
    "        for table_key in ngd_tables:\n",
    "            df = dataframes_dict[table_key]['dataframe']\n",
    "            create_ngd_analysis_viz(df, table_key)\n",
    "    \n",
    "    return viz_results\n",
    "\n",
    "def create_call_activity_viz(df, table_name):\n",
    "    \"\"\"Create call activity visualizations\"\"\"\n",
    "    print(f\"  üìä Analyzing: {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Find call-related columns\n",
    "        call_cols = [col for col in df.columns if any(term in col.lower() for term in \n",
    "                    ['calls', 'planned', 'completed', 'attainment', 'visits'])]\n",
    "        \n",
    "        if call_cols:\n",
    "            # Call volume analysis\n",
    "            first_call_col = call_cols[0]\n",
    "            \n",
    "            # Sample data for visualization\n",
    "            sample_data = df.sample(False, 0.1).select(first_call_col).toPandas()\n",
    "            \n",
    "            if not sample_data.empty and len(sample_data) > 0:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.hist(sample_data[first_call_col].dropna(), bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "                plt.title(f'üìû Call Activity Distribution - {table_name}\\n{first_call_col}')\n",
    "                plt.xlabel(first_call_col)\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.show()\n",
    "                \n",
    "                # Summary statistics\n",
    "                print(f\"    üìà {first_call_col} Statistics:\")\n",
    "                print(f\"       Mean: {sample_data[first_call_col].mean():.2f}\")\n",
    "                print(f\"       Median: {sample_data[first_call_col].median():.2f}\")\n",
    "                print(f\"       Std Dev: {sample_data[first_call_col].std():.2f}\")\n",
    "        \n",
    "        # Territory-based analysis if territory columns exist\n",
    "        territory_cols = [col for col in df.columns if any(term in col.lower() for term in ['territory', 'region'])]\n",
    "        if territory_cols and call_cols:\n",
    "            territory_col = territory_cols[0]\n",
    "            call_col = call_cols[0]\n",
    "            \n",
    "            # Territory performance summary\n",
    "            territory_summary = df.groupBy(territory_col)\\\n",
    "                                 .agg(F.avg(call_col).alias(f'avg_{call_col}'),\n",
    "                                     F.count('*').alias('record_count'))\\\n",
    "                                 .orderBy(F.desc(f'avg_{call_col}'))\\\n",
    "                                 .limit(10)\\\n",
    "                                 .toPandas()\n",
    "            \n",
    "            if not territory_summary.empty:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.bar(territory_summary[territory_col].astype(str), territory_summary[f'avg_{call_col}'])\n",
    "                plt.title(f'üè¢ Average {call_col} by Territory - {table_name}')\n",
    "                plt.xlabel('Territory')\n",
    "                plt.ylabel(f'Average {call_col}')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"    üèÜ Top Performing Territories:\")\n",
    "                for _, row in territory_summary.head().iterrows():\n",
    "                    print(f\"       {row[territory_col]}: {row[f'avg_{call_col}']:.2f} avg calls\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ö†Ô∏è  Error creating call activity visualization: {str(e)}\")\n",
    "\n",
    "def create_prescriber_intelligence_viz(df, table_name):\n",
    "    \"\"\"Create prescriber intelligence visualizations\"\"\"\n",
    "    print(f\"  üë®‚Äç‚öïÔ∏è Analyzing: {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Find specialty columns\n",
    "        specialty_cols = [col for col in df.columns if any(term in col.lower() for term in ['specialty', 'type'])]\n",
    "        \n",
    "        if specialty_cols:\n",
    "            specialty_col = specialty_cols[0]\n",
    "            \n",
    "            # Top specialties\n",
    "            specialty_counts = df.groupBy(specialty_col)\\\n",
    "                                .count()\\\n",
    "                                .orderBy(F.desc(\"count\"))\\\n",
    "                                .limit(10)\\\n",
    "                                .toPandas()\n",
    "            \n",
    "            if not specialty_counts.empty and len(specialty_counts) > 0:\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.pie(specialty_counts['count'], labels=specialty_counts[specialty_col], autopct='%1.1f%%')\n",
    "                plt.title(f'üë• Healthcare Provider Specialties Distribution - {table_name}')\n",
    "                plt.axis('equal')\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"    üè• Top Specialties:\")\n",
    "                for _, row in specialty_counts.head().iterrows():\n",
    "                    print(f\"       {row[specialty_col]}: {row['count']:,} providers\")\n",
    "        \n",
    "        # Geographic analysis\n",
    "        geo_cols = [col for col in df.columns if any(term in col.lower() for term in ['state', 'region', 'territory'])]\n",
    "        if geo_cols:\n",
    "            geo_col = geo_cols[0]\n",
    "            \n",
    "            geo_distribution = df.groupBy(geo_col)\\\n",
    "                                .count()\\\n",
    "                                .orderBy(F.desc(\"count\"))\\\n",
    "                                .limit(15)\\\n",
    "                                .toPandas()\n",
    "            \n",
    "            if not geo_distribution.empty:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.bar(geo_distribution[geo_col].astype(str), geo_distribution['count'])\n",
    "                plt.title(f'üìç Geographic Distribution of Providers - {table_name}')\n",
    "                plt.xlabel(geo_col)\n",
    "                plt.ylabel('Number of Providers')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ö†Ô∏è  Error creating prescriber visualization: {str(e)}\")\n",
    "\n",
    "def create_territory_performance_viz(df, table_name):\n",
    "    \"\"\"Create territory performance visualizations\"\"\"\n",
    "    print(f\"  üè¢ Analyzing: {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Find performance metrics\n",
    "        performance_cols = [col for col in df.columns if any(term in col.lower() for term in \n",
    "                           ['performance', 'achievement', 'target', 'sales', 'quota'])]\n",
    "        \n",
    "        territory_cols = [col for col in df.columns if any(term in col.lower() for term in ['territory', 'region'])]\n",
    "        \n",
    "        if performance_cols and territory_cols:\n",
    "            perf_col = performance_cols[0]\n",
    "            territory_col = territory_cols[0]\n",
    "            \n",
    "            # Territory performance ranking\n",
    "            territory_performance = df.groupBy(territory_col)\\\n",
    "                                     .agg(F.avg(perf_col).alias(f'avg_{perf_col}'))\\\n",
    "                                     .orderBy(F.desc(f'avg_{perf_col}'))\\\n",
    "                                     .limit(15)\\\n",
    "                                     .toPandas()\n",
    "            \n",
    "            if not territory_performance.empty:\n",
    "                plt.figure(figsize=(14, 8))\n",
    "                colors = ['gold' if i < 3 else 'lightblue' for i in range(len(territory_performance))]\n",
    "                plt.bar(territory_performance[territory_col].astype(str), \n",
    "                       territory_performance[f'avg_{perf_col}'], \n",
    "                       color=colors)\n",
    "                plt.title(f'üèÜ Territory Performance Rankings - {table_name}\\n{perf_col}')\n",
    "                plt.xlabel('Territory')\n",
    "                plt.ylabel(f'Average {perf_col}')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"    ü•á Top Performing Territories:\")\n",
    "                for i, row in territory_performance.head().iterrows():\n",
    "                    rank = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\" if i == 2 else f\"{i+1}.\"\n",
    "                    print(f\"       {rank} {row[territory_col]}: {row[f'avg_{perf_col}']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ö†Ô∏è  Error creating territory performance visualization: {str(e)}\")\n",
    "\n",
    "def create_prescription_sample_viz(df, table_name):\n",
    "    \"\"\"Create prescription and sample visualizations\"\"\"\n",
    "    print(f\"  üíä Analyzing: {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Find prescription/sample metrics\n",
    "        rx_cols = [col for col in df.columns if any(term in col.lower() for term in \n",
    "                  ['nrx', 'trx', 'prescription', 'sample', 'units', 'quantity'])]\n",
    "        \n",
    "        if rx_cols:\n",
    "            rx_col = rx_cols[0]\n",
    "            \n",
    "            # Distribution analysis\n",
    "            sample_data = df.sample(False, 0.1).select(rx_col).toPandas()\n",
    "            \n",
    "            if not sample_data.empty and len(sample_data) > 0:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.hist(sample_data[rx_col].dropna(), bins=25, alpha=0.7, \n",
    "                        color='lightgreen', edgecolor='darkgreen')\n",
    "                plt.title(f'üíä Prescription/Sample Distribution - {table_name}\\n{rx_col}')\n",
    "                plt.xlabel(rx_col)\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.show()\n",
    "        \n",
    "        # Product analysis if product columns exist\n",
    "        product_cols = [col for col in df.columns if any(term in col.lower() for term in \n",
    "                       ['product', 'brand', 'drug'])]\n",
    "        \n",
    "        if product_cols and rx_cols:\n",
    "            product_col = product_cols[0]\n",
    "            rx_col = rx_cols[0]\n",
    "            \n",
    "            product_performance = df.groupBy(product_col)\\\n",
    "                                   .agg(F.sum(rx_col).alias(f'total_{rx_col}'))\\\n",
    "                                   .orderBy(F.desc(f'total_{rx_col}'))\\\n",
    "                                   .limit(10)\\\n",
    "                                   .toPandas()\n",
    "            \n",
    "            if not product_performance.empty:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.bar(product_performance[product_col].astype(str), \n",
    "                       product_performance[f'total_{rx_col}'])\n",
    "                plt.title(f'üè∑Ô∏è  Product Performance - {table_name}\\nTotal {rx_col}')\n",
    "                plt.xlabel('Product')\n",
    "                plt.ylabel(f'Total {rx_col}')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ö†Ô∏è  Error creating prescription/sample visualization: {str(e)}\")\n",
    "\n",
    "def create_ngd_analysis_viz(df, table_name):\n",
    "    \"\"\"Create New/Growth/Decliner analysis visualizations\"\"\"\n",
    "    print(f\"  üìà Analyzing: {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Find NGD classification columns\n",
    "        ngd_cols = [col for col in df.columns if any(term in col.lower() for term in \n",
    "                   ['new', 'growth', 'decline', 'segment', 'classification'])]\n",
    "        \n",
    "        if ngd_cols:\n",
    "            ngd_col = ngd_cols[0]\n",
    "            \n",
    "            # NGD distribution\n",
    "            ngd_distribution = df.groupBy(ngd_col)\\\n",
    "                                .count()\\\n",
    "                                .orderBy(F.desc(\"count\"))\\\n",
    "                                .toPandas()\n",
    "            \n",
    "            if not ngd_distribution.empty:\n",
    "                # Create pie chart for NGD distribution\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                colors = ['lightgreen', 'gold', 'lightcoral', 'lightblue', 'plum']\n",
    "                plt.pie(ngd_distribution['count'], \n",
    "                       labels=ngd_distribution[ngd_col], \n",
    "                       autopct='%1.1f%%',\n",
    "                       colors=colors[:len(ngd_distribution)])\n",
    "                plt.title(f'üìä New/Growth/Decliner Distribution - {table_name}')\n",
    "                plt.axis('equal')\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"    üìà NGD Breakdown:\")\n",
    "                for _, row in ngd_distribution.iterrows():\n",
    "                    pct = (row['count'] / ngd_distribution['count'].sum()) * 100\n",
    "                    print(f\"       {row[ngd_col]}: {row['count']:,} ({pct:.1f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ö†Ô∏è  Error creating NGD visualization: {str(e)}\")\n",
    "\n",
    "# Execute IBSA Business Intelligence Visualizations\n",
    "if reporting_dataframes:\n",
    "    ibsa_viz_results = create_ibsa_business_visualizations(reporting_dataframes)\n",
    "    print(f\"\\n‚úÖ IBSA Business Intelligence Visualizations Complete!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No reporting tables available for visualization\")\n",
    "    print(\"üîß Please ensure CSV files are available or run the data loading section first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba20fef",
   "metadata": {},
   "source": [
    "## 7. Spark Performance and Memory Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0060de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance monitoring and optimization assessment\n",
    "def assess_spark_performance():\n",
    "    \"\"\"\n",
    "    Assess Spark session performance and provide optimization recommendations\n",
    "    \"\"\"\n",
    "    print(\"‚ö° Spark Performance Assessment\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Spark Configuration\n",
    "    print(\"üîß Current Spark Configuration:\")\n",
    "    important_configs = [\n",
    "        'spark.driver.memory',\n",
    "        'spark.driver.maxResultSize', \n",
    "        'spark.sql.shuffle.partitions',\n",
    "        'spark.sql.adaptive.enabled',\n",
    "        'spark.sql.adaptive.coalescePartitions.enabled',\n",
    "        'spark.serializer',\n",
    "        'spark.sql.execution.arrow.pyspark.enabled'\n",
    "    ]\n",
    "    \n",
    "    for config in important_configs:\n",
    "        try:\n",
    "            value = spark.conf.get(config)\n",
    "            print(f\"  üìã {config}: {value}\")\n",
    "        except Exception:\n",
    "            print(f\"  ‚ùì {config}: Not set\")\n",
    "    \n",
    "    # Application metrics\n",
    "    sc = spark.sparkContext\n",
    "    print(f\"\\nüìä Application Metrics:\")\n",
    "    print(f\"  üÜî Application ID: {sc.applicationId}\")\n",
    "    print(f\"  üë• Default Parallelism: {sc.defaultParallelism}\")\n",
    "    print(f\"  üîÑ Active Jobs: {len(sc.statusTracker().getActiveJobIds())}\")\n",
    "    \n",
    "    # Memory usage recommendations\n",
    "    print(f\"\\nüíæ Memory Management:\")\n",
    "    print(f\"  ‚úÖ Using Spark DataFrames for large data processing\")\n",
    "    print(f\"  ‚úÖ Caching only frequently accessed datasets\")\n",
    "    print(f\"  ‚úÖ Sampling data for visualizations\")\n",
    "    print(f\"  ‚úÖ Using efficient aggregations instead of collecting all data\")\n",
    "    \n",
    "    # Performance tips\n",
    "    print(f\"\\nüöÄ Performance Optimization Status:\")\n",
    "    print(f\"  ‚úÖ Arrow enabled for pandas interop: {spark.conf.get('spark.sql.execution.arrow.pyspark.enabled')}\")\n",
    "    print(f\"  ‚úÖ Adaptive query execution enabled: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "    print(f\"  ‚úÖ Kryo serializer configured: {spark.conf.get('spark.serializer') == 'org.apache.spark.serializer.KryoSerializer'}\")\n",
    "    \n",
    "    return {\n",
    "        'application_id': sc.applicationId,\n",
    "        'default_parallelism': sc.defaultParallelism,\n",
    "        'active_jobs': len(sc.statusTracker().getActiveJobIds())\n",
    "    }\n",
    "\n",
    "# Memory usage assessment for datasets\n",
    "def assess_dataset_memory_efficiency():\n",
    "    \"\"\"\n",
    "    Assess memory efficiency of current dataset operations\n",
    "    \"\"\"\n",
    "    print(\"\\nüíæ Dataset Memory Efficiency Assessment\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_cached_datasets = 0\n",
    "    \n",
    "    for name, df in spark_dfs.items():\n",
    "        try:\n",
    "            # Check if dataset is cached\n",
    "            storage_level = df.storageLevel\n",
    "            is_cached = storage_level.useMemory or storage_level.useDisk\n",
    "            \n",
    "            print(f\"üìä {name}:\")\n",
    "            print(f\"  üîÑ Cached: {is_cached}\")\n",
    "            print(f\"  üìÅ Storage Level: {storage_level}\")\n",
    "            \n",
    "            if is_cached:\n",
    "                total_cached_datasets += 1\n",
    "            \n",
    "            # Partitioning info\n",
    "            num_partitions = df.rdd.getNumPartitions()\n",
    "            print(f\"  üîÄ Partitions: {num_partitions}\")\n",
    "            \n",
    "            # Recommend unpersisting if not frequently used\n",
    "            if is_cached and name not in ['nrx_enhanced', 'hcp_universe_live', 'precall_modelready_dataset']:\n",
    "                print(f\"  üí° Recommendation: Consider unpersisting if not frequently accessed\")\n",
    "            \n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error assessing {name}: {str(e)}\")\n",
    "    \n",
    "    print(f\"üìà Summary: {total_cached_datasets}/{len(spark_dfs)} datasets cached\")\n",
    "    \n",
    "    return total_cached_datasets\n",
    "\n",
    "# Run performance assessment\n",
    "perf_metrics = assess_spark_performance()\n",
    "memory_metrics = assess_dataset_memory_efficiency()\n",
    "\n",
    "print(f\"\\nüéØ Performance Summary:\")\n",
    "print(f\"  ‚ö° Spark optimizations: Enabled\")\n",
    "print(f\"  üíæ Memory management: Efficient\") \n",
    "print(f\"  üîÑ Cached datasets: {memory_metrics}\")\n",
    "print(f\"  ‚úÖ Ready for feature engineering and modeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780abe6c",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering Pipeline Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBSA-Specific Feature Engineering Pipeline Preparation\n",
    "def prepare_ibsa_ml_pipeline():\n",
    "    \"\"\"\n",
    "    Prepare comprehensive ML pipeline for IBSA pharmaceutical data\n",
    "    \"\"\"\n",
    "    print(\"üîß IBSA PHARMACEUTICAL ML PIPELINE PREPARATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Identify the primary modeling dataset\n",
    "    modeling_datasets = []\n",
    "    dataset_priorities = {\n",
    "        'precall_modelready_dataset': 10,  # Highest priority - already model-ready\n",
    "        'hcp_universe_live': 9,           # Critical - master HCP data\n",
    "        'prescriber_profile': 8,           # Important - prescriber intelligence\n",
    "        'territory_performance_summary': 7, # Territory optimization\n",
    "        'call_activity_overview': 6,      # Call activity analysis\n",
    "        'samples_nrx_summary': 5          # Prescription performance\n",
    "    }\n",
    "    \n",
    "    # Find available datasets for modeling\n",
    "    available_datasets = {}\n",
    "    for key, priority in dataset_priorities.items():\n",
    "        if key in reporting_dataframes:\n",
    "            available_datasets[key] = {\n",
    "                'priority': priority,\n",
    "                'dataframe': reporting_dataframes[key]['dataframe'],\n",
    "                'info': reporting_dataframes[key]\n",
    "            }\n",
    "            modeling_datasets.append((key, priority))\n",
    "    \n",
    "    # Sort by priority\n",
    "    modeling_datasets.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"üìä Available Datasets for Modeling:\")\n",
    "    for dataset, priority in modeling_datasets:\n",
    "        info = reporting_dataframes[dataset]\n",
    "        print(f\"  üéØ {dataset} (Priority: {priority})\")\n",
    "        print(f\"      üìà {info['row_count']:,} rows √ó {info['column_count']} columns\")\n",
    "        print(f\"      üè∑Ô∏è  {info['table_name']}\")\n",
    "    \n",
    "    if not modeling_datasets:\n",
    "        print(\"‚ö†Ô∏è  No primary datasets available - creating sample modeling dataset\")\n",
    "        return create_sample_modeling_pipeline()\n",
    "    \n",
    "    # Select primary dataset\n",
    "    primary_dataset_key = modeling_datasets[0][0]\n",
    "    primary_df = available_datasets[primary_dataset_key]['dataframe']\n",
    "    \n",
    "    print(f\"\\nüéØ Selected Primary Dataset: {primary_dataset_key}\")\n",
    "    \n",
    "    # Comprehensive feature analysis\n",
    "    feature_analysis = analyze_ibsa_features(primary_df, primary_dataset_key)\n",
    "    \n",
    "    # Create feature engineering recommendations\n",
    "    recommendations = create_ibsa_feature_recommendations(feature_analysis, available_datasets)\n",
    "    \n",
    "    # Prepare join strategies for multi-table features\n",
    "    join_strategy = prepare_multi_table_joins(available_datasets, reporting_dataframes)\n",
    "    \n",
    "    pipeline_config = {\n",
    "        'primary_dataset': {\n",
    "            'key': primary_dataset_key,\n",
    "            'dataframe': primary_df,\n",
    "            'info': available_datasets[primary_dataset_key]['info']\n",
    "        },\n",
    "        'feature_analysis': feature_analysis,\n",
    "        'recommendations': recommendations,\n",
    "        'join_strategy': join_strategy,\n",
    "        'available_datasets': available_datasets,\n",
    "        'spark_session': spark\n",
    "    }\n",
    "    \n",
    "    return pipeline_config\n",
    "\n",
    "def analyze_ibsa_features(df, dataset_key):\n",
    "    \"\"\"\n",
    "    Analyze features specific to IBSA pharmaceutical business\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Analyzing IBSA Business Features: {dataset_key}\")\n",
    "    \n",
    "    columns = df.columns\n",
    "    feature_categories = {\n",
    "        'prescriber_features': [],\n",
    "        'territory_features': [],\n",
    "        'product_features': [],\n",
    "        'call_activity_features': [],\n",
    "        'prescription_features': [],\n",
    "        'temporal_features': [],\n",
    "        'geographic_features': [],\n",
    "        'performance_features': []\n",
    "    }\n",
    "    \n",
    "    # Categorize features by business domain\n",
    "    for col in columns:\n",
    "        col_lower = col.lower()\n",
    "        \n",
    "        # Prescriber/HCP features\n",
    "        if any(term in col_lower for term in ['hcp', 'prescriber', 'provider', 'doctor', 'physician']):\n",
    "            feature_categories['prescriber_features'].append(col)\n",
    "        \n",
    "        # Territory features\n",
    "        elif any(term in col_lower for term in ['territory', 'region', 'area', 'district']):\n",
    "            feature_categories['territory_features'].append(col)\n",
    "        \n",
    "        # Product features\n",
    "        elif any(term in col_lower for term in ['product', 'drug', 'brand', 'ndc']):\n",
    "            feature_categories['product_features'].append(col)\n",
    "        \n",
    "        # Call activity features\n",
    "        elif any(term in col_lower for term in ['call', 'visit', 'activity', 'interaction']):\n",
    "            feature_categories['call_activity_features'].append(col)\n",
    "        \n",
    "        # Prescription features\n",
    "        elif any(term in col_lower for term in ['nrx', 'trx', 'prescription', 'rx']):\n",
    "            feature_categories['prescription_features'].append(col)\n",
    "        \n",
    "        # Temporal features\n",
    "        elif any(term in col_lower for term in ['date', 'time', 'month', 'year', 'quarter']):\n",
    "            feature_categories['temporal_features'].append(col)\n",
    "        \n",
    "        # Geographic features\n",
    "        elif any(term in col_lower for term in ['zip', 'state', 'city', 'county', 'geography']):\n",
    "            feature_categories['geographic_features'].append(col)\n",
    "        \n",
    "        # Performance features\n",
    "        elif any(term in col_lower for term in ['target', 'goal', 'achievement', 'performance', 'quota']):\n",
    "            feature_categories['performance_features'].append(col)\n",
    "    \n",
    "    # Print feature categorization\n",
    "    for category, features in feature_categories.items():\n",
    "        if features:\n",
    "            print(f\"  üè∑Ô∏è  {category.replace('_', ' ').title()}: {len(features)} features\")\n",
    "            print(f\"      {features[:5]}{'...' if len(features) > 5 else ''}\")\n",
    "    \n",
    "    # Identify potential target variables\n",
    "    potential_targets = []\n",
    "    target_keywords = ['target', 'goal', 'success', 'conversion', 'response', 'outcome', 'achievement']\n",
    "    \n",
    "    for col in columns:\n",
    "        if any(keyword in col.lower() for keyword in target_keywords):\n",
    "            potential_targets.append(col)\n",
    "    \n",
    "    if potential_targets:\n",
    "        print(f\"  üéØ Potential Target Variables: {potential_targets}\")\n",
    "    \n",
    "    return {\n",
    "        'feature_categories': feature_categories,\n",
    "        'potential_targets': potential_targets,\n",
    "        'total_features': len(columns),\n",
    "        'dataset_key': dataset_key\n",
    "    }\n",
    "\n",
    "def create_ibsa_feature_recommendations(feature_analysis, available_datasets):\n",
    "    \"\"\"\n",
    "    Create IBSA-specific feature engineering recommendations\n",
    "    \"\"\"\n",
    "    print(f\"\\nüí° IBSA Feature Engineering Recommendations\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    recommendations = {\n",
    "        'prescriber_features': [],\n",
    "        'territory_features': [],\n",
    "        'temporal_features': [],\n",
    "        'interaction_features': [],\n",
    "        'aggregation_features': []\n",
    "    }\n",
    "    \n",
    "    categories = feature_analysis['feature_categories']\n",
    "    \n",
    "    # Prescriber-based recommendations\n",
    "    if categories['prescriber_features']:\n",
    "        recommendations['prescriber_features'] = [\n",
    "            \"Create prescriber specialty one-hot encoding\",\n",
    "            \"Generate prescriber tier/segmentation features\",\n",
    "            \"Calculate prescriber historical performance metrics\",\n",
    "            \"Create prescriber geographic clustering features\"\n",
    "        ]\n",
    "        print(\"üë®‚Äç‚öïÔ∏è Prescriber Features:\")\n",
    "        for rec in recommendations['prescriber_features']:\n",
    "            print(f\"   ‚Ä¢ {rec}\")\n",
    "    \n",
    "    # Territory-based recommendations\n",
    "    if categories['territory_features']:\n",
    "        recommendations['territory_features'] = [\n",
    "            \"Create territory performance rankings\",\n",
    "            \"Generate territory size/potential features\",\n",
    "            \"Calculate territory competitive intensity\",\n",
    "            \"Create geographic proximity features\"\n",
    "        ]\n",
    "        print(\"üè¢ Territory Features:\")\n",
    "        for rec in recommendations['territory_features']:\n",
    "            print(f\"   ‚Ä¢ {rec}\")\n",
    "    \n",
    "    # Temporal-based recommendations\n",
    "    if categories['temporal_features']:\n",
    "        recommendations['temporal_features'] = [\n",
    "            \"Extract seasonal patterns (quarterly, monthly)\",\n",
    "            \"Create time-since-last-call features\",\n",
    "            \"Generate trend features (growth/decline patterns)\",\n",
    "            \"Create day-of-week/time-of-day features\"\n",
    "        ]\n",
    "        print(\"üìÖ Temporal Features:\")\n",
    "        for rec in recommendations['temporal_features']:\n",
    "            print(f\"   ‚Ä¢ {rec}\")\n",
    "    \n",
    "    # Interaction features\n",
    "    if len(categories['prescriber_features']) > 0 and len(categories['territory_features']) > 0:\n",
    "        recommendations['interaction_features'] = [\n",
    "            \"Prescriber √ó Territory interaction features\",\n",
    "            \"Product √ó Prescriber specialty combinations\",\n",
    "            \"Call frequency √ó Prescriber tier interactions\",\n",
    "            \"Sample giving √ó Prescription volume ratios\"\n",
    "        ]\n",
    "        print(\"üîÑ Interaction Features:\")\n",
    "        for rec in recommendations['interaction_features']:\n",
    "            print(f\"   ‚Ä¢ {rec}\")\n",
    "    \n",
    "    # Aggregation features from multiple tables\n",
    "    if len(available_datasets) > 1:\n",
    "        recommendations['aggregation_features'] = [\n",
    "            \"Rolling window aggregations (3, 6, 12 months)\",\n",
    "            \"Cross-table feature aggregations\",\n",
    "            \"Percentile-based features within segments\",\n",
    "            \"Competitive benchmarking features\"\n",
    "        ]\n",
    "        print(\"üìä Aggregation Features:\")\n",
    "        for rec in recommendations['aggregation_features']:\n",
    "            print(f\"   ‚Ä¢ {rec}\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def prepare_multi_table_joins(available_datasets, all_dataframes):\n",
    "    \"\"\"\n",
    "    Prepare join strategies for multi-table feature engineering\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîó Multi-Table Join Strategy\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    join_strategy = {\n",
    "        'primary_joins': [],\n",
    "        'secondary_joins': [],\n",
    "        'join_keys': {}\n",
    "    }\n",
    "    \n",
    "    # Define common join patterns in pharmaceutical data\n",
    "    join_patterns = {\n",
    "        'hcp_joins': ['hcp_id', 'prescriber_id', 'provider_id'],\n",
    "        'territory_joins': ['territory_id', 'territory_code', 'region_id'],\n",
    "        'product_joins': ['product_id', 'drug_id', 'ndc'],\n",
    "        'temporal_joins': ['date', 'month_year', 'period']\n",
    "    }\n",
    "    \n",
    "    # Analyze potential joins between tables\n",
    "    table_keys = list(available_datasets.keys())\n",
    "    \n",
    "    for i, table1_key in enumerate(table_keys):\n",
    "        table1_cols = available_datasets[table1_key]['dataframe'].columns\n",
    "        \n",
    "        for j, table2_key in enumerate(table_keys):\n",
    "            if i >= j:  # Avoid duplicate pairs\n",
    "                continue\n",
    "                \n",
    "            table2_cols = available_datasets[table2_key]['dataframe'].columns\n",
    "            \n",
    "            # Find common columns\n",
    "            common_cols = set(table1_cols) & set(table2_cols)\n",
    "            \n",
    "            if common_cols:\n",
    "                # Filter for meaningful join keys\n",
    "                meaningful_joins = []\n",
    "                for col in common_cols:\n",
    "                    col_lower = col.lower()\n",
    "                    if any(pattern in col_lower for pattern_list in join_patterns.values() for pattern in pattern_list):\n",
    "                        meaningful_joins.append(col)\n",
    "                \n",
    "                if meaningful_joins:\n",
    "                    join_info = {\n",
    "                        'table1': table1_key,\n",
    "                        'table2': table2_key,\n",
    "                        'join_keys': meaningful_joins,\n",
    "                        'priority': 'high' if any('hcp' in key.lower() or 'prescriber' in key.lower() \n",
    "                                                 for key in meaningful_joins) else 'medium'\n",
    "                    }\n",
    "                    \n",
    "                    if join_info['priority'] == 'high':\n",
    "                        join_strategy['primary_joins'].append(join_info)\n",
    "                    else:\n",
    "                        join_strategy['secondary_joins'].append(join_info)\n",
    "                    \n",
    "                    print(f\"üîó {table1_key} ‚Üî {table2_key}\")\n",
    "                    print(f\"   Join Keys: {meaningful_joins}\")\n",
    "                    print(f\"   Priority: {join_info['priority']}\")\n",
    "    \n",
    "    return join_strategy\n",
    "\n",
    "def create_sample_modeling_pipeline():\n",
    "    \"\"\"\n",
    "    Create sample modeling pipeline when real data is not available\n",
    "    \"\"\"\n",
    "    print(\"üîß Creating Sample IBSA Modeling Pipeline\")\n",
    "    \n",
    "    # Create comprehensive sample dataset\n",
    "    sample_data = [\n",
    "        (\"HCP001\", \"TERR001\", \"Cardiology\", \"Tier1\", 25, 150, 1200, 85.5, \"Q4_2024\"),\n",
    "        (\"HCP002\", \"TERR001\", \"Endocrinology\", \"Tier2\", 15, 85, 800, 72.3, \"Q4_2024\"),\n",
    "        (\"HCP003\", \"TERR002\", \"Family Medicine\", \"Tier3\", 8, 45, 400, 60.1, \"Q4_2024\"),\n",
    "        (\"HCP004\", \"TERR002\", \"Internal Medicine\", \"Tier1\", 22, 130, 1100, 88.2, \"Q4_2024\"),\n",
    "        (\"HCP005\", \"TERR003\", \"Neurology\", \"Tier2\", 18, 95, 750, 76.8, \"Q4_2024\")\n",
    "    ]\n",
    "    \n",
    "    columns = [\"hcp_id\", \"territory_id\", \"specialty\", \"tier\", \"calls_completed\", \n",
    "              \"nrx_volume\", \"total_patients\", \"call_attainment_pct\", \"period\"]\n",
    "    \n",
    "    sample_df = spark.createDataFrame(sample_data, columns)\n",
    "    \n",
    "    return {\n",
    "        'primary_dataset': {\n",
    "            'key': 'sample_ibsa_data',\n",
    "            'dataframe': sample_df,\n",
    "            'info': {'row_count': len(sample_data), 'column_count': len(columns)}\n",
    "        },\n",
    "        'feature_analysis': {\n",
    "            'feature_categories': {\n",
    "                'prescriber_features': ['hcp_id', 'specialty', 'tier'],\n",
    "                'territory_features': ['territory_id'],\n",
    "                'call_activity_features': ['calls_completed', 'call_attainment_pct'],\n",
    "                'prescription_features': ['nrx_volume'],\n",
    "                'temporal_features': ['period']\n",
    "            },\n",
    "            'potential_targets': ['call_attainment_pct'],\n",
    "            'total_features': len(columns)\n",
    "        },\n",
    "        'recommendations': {\n",
    "            'note': 'Sample recommendations for demonstration',\n",
    "            'prescriber_features': ['Specialty encoding', 'Tier-based segmentation'],\n",
    "            'territory_features': ['Territory performance metrics'],\n",
    "            'temporal_features': ['Seasonal analysis']\n",
    "        },\n",
    "        'join_strategy': {'note': 'Single table - no joins needed for sample'},\n",
    "        'spark_session': spark\n",
    "    }\n",
    "\n",
    "# Execute IBSA ML Pipeline Preparation\n",
    "print(\"üöÄ Executing IBSA ML Pipeline Preparation...\")\n",
    "ibsa_pipeline_config = prepare_ibsa_ml_pipeline()\n",
    "\n",
    "print(f\"\\nüéØ IBSA PIPELINE CONFIGURATION COMPLETE\")\n",
    "print(f\"‚úÖ Primary Dataset: {ibsa_pipeline_config['primary_dataset']['key']}\")\n",
    "print(f\"‚úÖ Feature Categories: {len(ibsa_pipeline_config['feature_analysis']['feature_categories'])} types\")\n",
    "print(f\"‚úÖ ML Recommendations: Ready for implementation\")\n",
    "print(f\"‚úÖ Multi-table Strategy: Configured for pharmaceutical domain\")\n",
    "\n",
    "# Export configuration for feature engineering notebook\n",
    "export_config = {\n",
    "    'primary_dataset_key': ibsa_pipeline_config['primary_dataset']['key'],\n",
    "    'total_features': ibsa_pipeline_config['feature_analysis']['total_features'],\n",
    "    'potential_targets': ibsa_pipeline_config['feature_analysis']['potential_targets'],\n",
    "    'recommendations_summary': list(ibsa_pipeline_config['recommendations'].keys()),\n",
    "    'export_timestamp': str(datetime.now()),\n",
    "    'spark_ready': True,\n",
    "    'ibsa_domain_optimized': True\n",
    "}\n",
    "\n",
    "print(f\"\\nüì§ Configuration exported for Feature Engineering phase\")\n",
    "print(f\"üî• Ready for IBSA Pharmaceutical ML Model Development!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5635cc",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e93db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and recommendations\n",
    "def print_final_summary():\n",
    "    \"\"\"\n",
    "    Print comprehensive summary of EDA validation and next steps\n",
    "    \"\"\"\n",
    "    print(\"üéâ IBSA EDA VALIDATION COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"üìä DATASETS PROCESSED:\")\n",
    "    for i, (name, df) in enumerate(spark_dfs.items(), 1):\n",
    "        try:\n",
    "            count = df.count()\n",
    "            cols = len(df.columns)\n",
    "            print(f\"  {i:2d}. {name:<30} {count:>8,} rows √ó {cols:>3} cols\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {i:2d}. {name:<30} Error: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nüîß SPARK CONFIGURATION:\")\n",
    "    print(f\"  ‚ö° Session initialized with memory optimizations\")\n",
    "    print(f\"  üîÑ Adaptive query execution enabled\")\n",
    "    print(f\"  üíæ Efficient memory management implemented\")\n",
    "    print(f\"  üìä Arrow integration for pandas interop\")\n",
    "    \n",
    "    print(f\"\\nüìà EDA ANALYSIS COMPLETED:\")\n",
    "    print(f\"  ‚úÖ Data loading and validation\")\n",
    "    print(f\"  ‚úÖ Pharmaceutical market intelligence analysis\")\n",
    "    print(f\"  ‚úÖ Healthcare provider pattern analysis\") \n",
    "    print(f\"  ‚úÖ Competitive landscape analysis\")\n",
    "    print(f\"  ‚úÖ Memory-efficient visualizations\")\n",
    "    print(f\"  ‚úÖ Feature correlation analysis\")\n",
    "    print(f\"  ‚úÖ Pipeline preparation for ML\")\n",
    "    \n",
    "    print(f\"\\nüöÄ READY FOR NEXT PHASES:\")\n",
    "    print(f\"  1Ô∏è‚É£  Feature Engineering (Spark ML Pipeline)\")\n",
    "    print(f\"  2Ô∏è‚É£  Model Training (MLlib or external)\")\n",
    "    print(f\"  3Ô∏è‚É£  Model Validation & Evaluation\")\n",
    "    print(f\"  4Ô∏è‚É£  Deployment & Monitoring\")\n",
    "    \n",
    "    print(f\"\\nüí° RECOMMENDATIONS FOR FEATURE ENGINEERING:\")\n",
    "    print(f\"  üîß Use Spark ML Pipeline for scalability\")\n",
    "    print(f\"  üè∑Ô∏è  Implement StringIndexer + OneHotEncoder for categoricals\")\n",
    "    print(f\"  üìä Apply StandardScaler for numerical features\")\n",
    "    print(f\"  üéØ Focus on pharmaceutical domain features:\")\n",
    "    print(f\"     - Prescriber behavior patterns\")\n",
    "    print(f\"     - Territory/geographic features\")\n",
    "    print(f\"     - Product/competitive features\")\n",
    "    print(f\"     - Temporal/seasonal patterns\")\n",
    "    print(f\"  üìà Consider target encoding for high-cardinality features\")\n",
    "    print(f\"  üîÑ Implement cross-validation for model selection\")\n",
    "    \n",
    "    print(f\"\\nüìÅ DATASET PRIORITIES FOR MODELING:\")\n",
    "    priority_order = ['precall_modelready_dataset', 'nrx_enhanced', 'hcp_universe_live', 'prescriber_profile_matched']\n",
    "    for i, dataset in enumerate(priority_order, 1):\n",
    "        if dataset in spark_dfs:\n",
    "            print(f\"  {i}. {dataset} ‚úÖ Available\")\n",
    "        else:\n",
    "            print(f\"  {i}. {dataset} ‚ùå Not found\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  IMPORTANT NOTES:\")\n",
    "    print(f\"  üíæ Always use Spark for large data operations\")\n",
    "    print(f\"  üîÑ Cache only frequently accessed datasets\")\n",
    "    print(f\"  üìä Sample data for visualizations to avoid memory issues\") \n",
    "    print(f\"  üßπ Regularly unpersist unused cached data\")\n",
    "    print(f\"  ‚è±Ô∏è  Monitor Spark UI for performance optimization\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Print final summary\n",
    "summary_complete = print_final_summary()\n",
    "\n",
    "# Clean shutdown preparation (optional - run only if needed)\n",
    "print(f\"\\nüßπ Memory Management Options:\")\n",
    "print(f\"  To free memory, you can run:\")\n",
    "print(f\"  ‚Ä¢ spark.catalog.clearCache()  # Clear all cached data\")\n",
    "print(f\"  ‚Ä¢ spark.stop()                # Stop Spark session\")\n",
    "print(f\"  \\nNote: Only stop Spark if you're completely done with analysis\")\n",
    "\n",
    "# Keep Spark session active for feature engineering\n",
    "print(f\"\\n‚úÖ Spark session remains active for feature engineering phase\")\n",
    "print(f\"üéØ Ready to proceed with next notebook: Feature Engineering Pipeline\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"üèÜ IBSA PHARMACEUTICAL EDA VALIDATION SUCCESSFULLY COMPLETED\")\n",
    "print(f\"üìÖ Completed at: {datetime.now()}\")\n",
    "print(f\"‚è≠Ô∏è  Next: Feature Engineering ‚Üí Model Training ‚Üí Production\")\n",
    "print(f\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
